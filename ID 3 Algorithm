import pandas as pd
import numpy as np
from collections import Counter
from math import log2

# Load the dataset
def load_data(filename):
    """Reads the CSV file and returns it as a DataFrame."""
    return pd.read_csv(filename)

# Calculate entropy
def entropy(labels):
    """Calculates entropy of a set of labels."""
    total = len(labels)
    label_counts = Counter(labels)
    return -sum((count / total) * log2(count / total) for count in label_counts.values() if count > 0)

# Calculate information gain
def information_gain(data, attribute, target_attribute):
    """Calculates information gain of splitting the data on a given attribute."""
    total_entropy = entropy(data[target_attribute])
    values = data[attribute].unique()
    weighted_entropy = sum((len(data[data[attribute] == v]) / len(data)) * entropy(data[data[attribute] == v][target_attribute]) for v in values)
    return total_entropy - weighted_entropy

# ID3 algorithm to build the decision tree
def id3(data, target_attribute, attributes):
    labels = data[target_attribute]
    
    # Base cases
    if len(set(labels)) == 1:  # All labels are the same
        return labels.iloc[0]
    if not attributes:  # No more attributes to split
        return labels.mode()[0]  # Return the most common label

    # Choose the best attribute to split on
    gains = {attr: information_gain(data, attr, target_attribute) for attr in attributes}
    best_attr = max(gains, key=gains.get)

    # Create the decision tree
    tree = {best_attr: {}}
    for value in data[best_attr].unique():
        subset = data[data[best_attr] == value].drop(columns=[best_attr])
        subtree = id3(subset, target_attribute, [attr for attr in attributes if attr != best_attr])
        tree[best_attr][value] = subtree

    return tree

# Classify a new sample using the decision tree
def classify(tree, sample):
    if not isinstance(tree, dict):
        return tree  # Return leaf node
    
    attribute = next(iter(tree))  # Get the next attribute to check
    if attribute not in sample:
        return None  # Attribute not in sample
    
    attribute_value = sample[attribute]
    subtree = tree[attribute].get(attribute_value)
    if subtree is None:
        return None  # Return None if the attribute value doesn't exist in the tree
    return classify(subtree, sample)

# Display the decision tree
def print_tree(tree, indent=""):
    """Pretty prints the decision tree structure."""
    if not isinstance(tree, dict):
        print(indent + "->", tree)
        return
    for attribute, subtree in tree.items():
        print(indent + str(attribute))
        for value, sub in subtree.items():
            print(indent + f"  [{value}]")
            print_tree(sub, indent + "    ")

# Main execution
if __name__ == "__main__":
    filename = '/content/decision tree.csv'  # Ensure this file exists in the same directory or provide a full path
    target_attribute = 'class'  # Set the target attribute name according to your dataset
    data = load_data(filename)
    
    # Attributes for building the tree (excluding target attribute)
    attributes = [col for col in data.columns if col != target_attribute]
    
    # Build the decision tree
    decision_tree = id3(data, target_attribute, attributes)
    
    print("Decision Tree:")
    print_tree(decision_tree)

    # Classify a new sample
    new_sample = {'attribute1': 'value1', 'attribute2': 'value2', 'attribute3': 'value3'}  # Replace with actual values that match your dataset
    prediction = classify(decision_tree, new_sample)
    
    print("\nClassified as:", prediction)
