import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Generate a synthetic dataset with a quadratic relationship
np.random.seed(42)
X = np.linspace(-6, 6, 100).reshape(-1, 1)  # 100 data points between -6 and 6
y = 2 * X**2 + 3 * X + 5 + np.random.normal(0, 3, size=(100, 1))  # Quadratic relationship with noise

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predictions using Linear Regression
y_pred_linear = linear_model.predict(X_test)

# Polynomial Regression Model (Degree 2)
poly_features = PolynomialFeatures(degree=2)
X_poly_train = poly_features.fit_transform(X_train)
X_poly_test = poly_features.transform(X_test)

polynomial_model = LinearRegression()
polynomial_model.fit(X_poly_train, y_train)

# Predictions using Polynomial Regression
y_pred_poly = polynomial_model.predict(X_poly_test)

# Calculate evaluation metrics for Linear Regression
mse_linear = mean_squared_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)

# Calculate evaluation metrics for Polynomial Regression
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

# Print results
print(f"Linear Regression: Mean Squared Error = {mse_linear:.2f}, R-squared = {r2_linear:.2f}")
print(f"Polynomial Regression: Mean Squared Error = {mse_poly:.2f}, R-squared = {r2_poly:.2f}")

# Plotting results
plt.figure(figsize=(14, 7))

# Plot Linear Regression
plt.subplot(1, 2, 1)
plt.scatter(X_test, y_test, color='blue', label='True Values')
plt.plot(X_test, y_pred_linear, color='red', label='Linear Regression')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()

# Plot Polynomial Regression
plt.subplot(1, 2, 2)
plt.scatter(X_test, y_test, color='blue', label='True Values')
X_range = np.linspace(-6, 6, 100).reshape(-1, 1)
X_range_poly = poly_features.transform(X_range)
y_range_poly = polynomial_model.predict(X_range_poly)
plt.plot(X_range, y_range_poly, color='green', label='Polynomial Regression (Degree 2)')
plt.title('Polynomial Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()

plt.tight_layout()
plt.show()
